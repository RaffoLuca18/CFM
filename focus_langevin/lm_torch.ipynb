{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1210747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import data_gen as dg\n",
    "import ising as isg\n",
    "\n",
    "import torch\n",
    "import geomloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99673cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_grad_potential(x, mu, L):\n",
    "    diff = x - mu\n",
    "\n",
    "    if diff.ndim == 1:\n",
    "        diff = diff.unsqueeze(1)  # shape (d, 1)\n",
    "        v1 = torch.linalg.solve_triangular(L, diff, upper=False)\n",
    "        v = torch.linalg.solve_triangular(L.T, v1, upper=True)\n",
    "        return v.squeeze(1)  # ritorna a shape (d,)\n",
    "    else:\n",
    "        # batch mode: x.shape = (n, d)\n",
    "        v1 = torch.linalg.solve_triangular(L, diff.T, upper=False)\n",
    "        v = torch.linalg.solve_triangular(L.T, v1, upper=True)\n",
    "        return v.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "807dc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def langevin_step(x, mu, L, eps):\n",
    "\n",
    "    noise = torch.randn_like(x)\n",
    "    grad_V = gaussian_grad_potential(x, mu, L)\n",
    "    \n",
    "    return x - eps * grad_V + torch.sqrt(torch.tensor(2.0 * eps)) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b7dea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_one_sample(x, mu, L, eps, n_evolution):\n",
    "    \n",
    "    for _ in range(n_evolution):\n",
    "        noise = torch.randn_like(x)\n",
    "        grad_V = gaussian_grad_potential(x, mu, L)\n",
    "        x = x - eps * grad_V + torch.sqrt(torch.tensor(2.0 * eps)) * noise\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32a8f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(samples, mu, L, eps=1e-2, n_evolution=10, seed=0):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    evolved_samples = []\n",
    "\n",
    "    for i in range(samples.shape[0]):\n",
    "        x = samples[i]\n",
    "        evolved = evolve_one_sample(x, mu, L, eps, n_evolution)\n",
    "        evolved_samples.append(evolved)\n",
    "\n",
    "    return torch.stack(evolved_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6b15f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(samples, mu, L, eps=1e-2, n_evolution=10, seed=0):\n",
    "\n",
    "    evolved_samples = evolve(samples, mu, L, eps=eps, n_evolution=n_evolution, seed=seed)\n",
    "    loss_fn = geomloss.SamplesLoss(\"sinkhorn\", p=2, blur=0.05)\n",
    "    \n",
    "    return loss_fn(samples, evolved_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc784901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lm(samples, n_steps=1000, lr=1e-2, eps=1e-2, n_evolution=10, seed=0, verbose=True):\n",
    "    n_samples, d = samples.shape\n",
    "    mu = torch.nn.Parameter(torch.zeros(d))\n",
    "    L_raw = torch.nn.Parameter(torch.eye(d))\n",
    "\n",
    "    optimizer = torch.optim.Adam([mu, L_raw], lr=lr)\n",
    "    loss_history = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        L = torch.tril(L_raw) + torch.eye(d) * 1e-3\n",
    "        loss = lm_loss(samples, mu, L, eps=eps, n_evolution=n_evolution, seed=seed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(loss.item())\n",
    "        if verbose and step % 1 == 0:\n",
    "            print(f\"Step {step} | Loss = {loss.item():.6f}\")\n",
    "    \n",
    "    return {\"mu\": mu.detach(), \"L\": torch.tril(L_raw).detach()}, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f4a69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_params(d, sigma_mu=0.1, sigma_cov=0.2, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    mu = sigma_mu * torch.randn(d)\n",
    "    A = sigma_cov * torch.randn(d, d)\n",
    "    cov = A @ A.T + 1e-2 * torch.eye(d)\n",
    "    return mu, cov\n",
    "\n",
    "def generate_gaussian_data(mu, cov, n_samples, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    L = torch.linalg.cholesky(cov)\n",
    "    z = torch.randn(n_samples, mu.shape[0])\n",
    "    return mu + z @ L.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce622492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ottimizzazione in corso...\n",
      "Step 0 | Loss = 0.180140\n",
      "Step 1 | Loss = 0.179333\n",
      "Step 2 | Loss = 0.178496\n",
      "Step 3 | Loss = 0.177623\n",
      "Step 4 | Loss = 0.176709\n",
      "Step 5 | Loss = 0.175753\n",
      "Step 6 | Loss = 0.174753\n",
      "Step 7 | Loss = 0.173706\n",
      "Step 8 | Loss = 0.172611\n",
      "Step 9 | Loss = 0.171465\n",
      "Step 10 | Loss = 0.170268\n",
      "Step 11 | Loss = 0.169016\n",
      "Step 12 | Loss = 0.167710\n",
      "Step 13 | Loss = 0.166348\n",
      "Step 14 | Loss = 0.164931\n",
      "Step 15 | Loss = 0.163457\n",
      "Step 16 | Loss = 0.161927\n",
      "Step 17 | Loss = 0.160342\n",
      "Step 18 | Loss = 0.158706\n",
      "Step 19 | Loss = 0.157020\n",
      "Step 20 | Loss = 0.155286\n",
      "Step 21 | Loss = 0.153505\n",
      "Step 22 | Loss = 0.151688\n",
      "Step 23 | Loss = 0.149841\n",
      "Step 24 | Loss = 0.147965\n",
      "Step 25 | Loss = 0.146055\n",
      "Step 26 | Loss = 0.144100\n",
      "Step 27 | Loss = 0.142099\n",
      "Step 28 | Loss = 0.140009\n",
      "Step 29 | Loss = 0.137808\n",
      "Step 30 | Loss = 0.135484\n",
      "Step 31 | Loss = 0.133015\n",
      "Step 32 | Loss = 0.130418\n",
      "Step 33 | Loss = 0.127744\n",
      "Step 34 | Loss = 0.125054\n",
      "Step 35 | Loss = 0.122419\n",
      "Step 36 | Loss = 0.119928\n",
      "Step 37 | Loss = 0.117625\n",
      "Step 38 | Loss = 0.115521\n",
      "Step 39 | Loss = 0.113545\n",
      "Step 40 | Loss = 0.111763\n",
      "Step 41 | Loss = 0.110054\n",
      "Step 42 | Loss = 0.108368\n",
      "Step 43 | Loss = 0.106767\n",
      "Step 44 | Loss = 0.105206\n",
      "Step 45 | Loss = 0.103652\n",
      "Step 46 | Loss = 0.102064\n",
      "Step 47 | Loss = 0.100419\n",
      "Step 48 | Loss = 0.098713\n",
      "Step 49 | Loss = 0.096934\n",
      "Step 50 | Loss = 0.095096\n",
      "Step 51 | Loss = 0.093232\n",
      "Step 52 | Loss = 0.091372\n",
      "Step 53 | Loss = 0.089547\n",
      "Step 54 | Loss = 0.087770\n",
      "Step 55 | Loss = 0.086081\n",
      "Step 56 | Loss = 0.084469\n",
      "Step 57 | Loss = 0.082947\n",
      "Step 58 | Loss = 0.081470\n",
      "Step 59 | Loss = 0.080010\n",
      "Step 60 | Loss = 0.078834\n",
      "Step 61 | Loss = 0.077302\n",
      "Step 62 | Loss = 0.075814\n",
      "Step 63 | Loss = 0.074365\n",
      "Step 64 | Loss = 0.072916\n",
      "Step 65 | Loss = 0.071435\n",
      "Step 66 | Loss = 0.069941\n",
      "Step 67 | Loss = 0.068552\n",
      "Step 68 | Loss = 0.067325\n",
      "Step 69 | Loss = 0.066269\n",
      "Step 70 | Loss = 0.065366\n",
      "Step 71 | Loss = 0.064627\n",
      "Step 72 | Loss = 0.064065\n",
      "Step 73 | Loss = 0.063681\n",
      "Step 74 | Loss = 0.063422\n",
      "Step 75 | Loss = 0.063177\n",
      "Step 76 | Loss = 0.062881\n",
      "Step 77 | Loss = 0.062488\n",
      "Step 78 | Loss = 0.062011\n",
      "Step 79 | Loss = 0.061506\n",
      "Step 80 | Loss = 0.061054\n",
      "Step 81 | Loss = 0.060651\n",
      "Step 82 | Loss = 0.060312\n",
      "Step 83 | Loss = 0.060046\n",
      "Step 84 | Loss = 0.059840\n",
      "Step 85 | Loss = 0.059687\n",
      "Step 86 | Loss = 0.059580\n",
      "Step 87 | Loss = 0.059497\n",
      "Step 88 | Loss = 0.059416\n",
      "Step 89 | Loss = 0.059332\n",
      "Step 90 | Loss = 0.059259\n",
      "Step 91 | Loss = 0.059206\n",
      "Step 92 | Loss = 0.059163\n",
      "Step 93 | Loss = 0.059112\n",
      "Step 94 | Loss = 0.059047\n",
      "Step 95 | Loss = 0.058963\n",
      "Step 96 | Loss = 0.058857\n",
      "Step 97 | Loss = 0.058720\n",
      "Step 98 | Loss = 0.058551\n",
      "Step 99 | Loss = 0.058356\n",
      "Step 100 | Loss = 0.058150\n",
      "Step 101 | Loss = 0.057939\n",
      "Step 102 | Loss = 0.057729\n",
      "Step 103 | Loss = 0.057525\n",
      "Step 104 | Loss = 0.057328\n",
      "Step 105 | Loss = 0.057134\n",
      "Step 106 | Loss = 0.056937\n",
      "Step 107 | Loss = 0.056736\n",
      "Step 108 | Loss = 0.056531\n",
      "Step 109 | Loss = 0.056326\n",
      "Step 110 | Loss = 0.056122\n",
      "Step 111 | Loss = 0.055919\n",
      "Step 112 | Loss = 0.055715\n",
      "Step 113 | Loss = 0.055508\n",
      "Step 114 | Loss = 0.055293\n",
      "Step 115 | Loss = 0.055068\n",
      "Step 116 | Loss = 0.054829\n",
      "Step 117 | Loss = 0.054580\n",
      "Step 118 | Loss = 0.054323\n",
      "Step 119 | Loss = 0.054063\n",
      "Step 120 | Loss = 0.053803\n",
      "Step 121 | Loss = 0.053544\n",
      "Step 122 | Loss = 0.053286\n",
      "Step 123 | Loss = 0.053031\n",
      "Step 124 | Loss = 0.052776\n",
      "Step 125 | Loss = 0.052522\n",
      "Step 126 | Loss = 0.052267\n",
      "Step 127 | Loss = 0.052012\n",
      "Step 128 | Loss = 0.051758\n",
      "Step 129 | Loss = 0.051505\n",
      "Step 130 | Loss = 0.051254\n",
      "Step 131 | Loss = 0.051007\n",
      "Step 132 | Loss = 0.050767\n",
      "Step 133 | Loss = 0.050536\n",
      "Step 134 | Loss = 0.050317\n",
      "Step 135 | Loss = 0.050112\n",
      "Step 136 | Loss = 0.049923\n",
      "Step 137 | Loss = 0.049750\n",
      "Step 138 | Loss = 0.049597\n",
      "Step 139 | Loss = 0.049466\n",
      "Step 140 | Loss = 0.049359\n",
      "Step 141 | Loss = 0.049276\n",
      "Step 142 | Loss = 0.049213\n",
      "Step 143 | Loss = 0.049168\n",
      "Step 144 | Loss = 0.049139\n",
      "Step 145 | Loss = 0.049122\n",
      "Step 146 | Loss = 0.049115\n",
      "Step 147 | Loss = 0.049116\n",
      "Step 148 | Loss = 0.049123\n",
      "Step 149 | Loss = 0.049133\n",
      "Step 150 | Loss = 0.049146\n",
      "Step 151 | Loss = 0.049161\n",
      "Step 152 | Loss = 0.049177\n",
      "Step 153 | Loss = 0.049195\n",
      "Step 154 | Loss = 0.049213\n",
      "Step 155 | Loss = 0.049232\n",
      "Step 156 | Loss = 0.049251\n",
      "Step 157 | Loss = 0.049268\n",
      "Step 158 | Loss = 0.049283\n",
      "Step 159 | Loss = 0.049298\n",
      "Step 160 | Loss = 0.049312\n",
      "Step 161 | Loss = 0.049326\n",
      "Step 162 | Loss = 0.049338\n",
      "Step 163 | Loss = 0.049350\n",
      "Step 164 | Loss = 0.049359\n",
      "Step 165 | Loss = 0.049365\n",
      "Step 166 | Loss = 0.049367\n",
      "Step 167 | Loss = 0.049365\n",
      "Step 168 | Loss = 0.049359\n",
      "Step 169 | Loss = 0.049351\n",
      "Step 170 | Loss = 0.049340\n",
      "Step 171 | Loss = 0.049329\n",
      "Step 172 | Loss = 0.049317\n",
      "Step 173 | Loss = 0.049306\n",
      "Step 174 | Loss = 0.049296\n",
      "Step 175 | Loss = 0.049285\n",
      "Step 176 | Loss = 0.049275\n",
      "Step 177 | Loss = 0.049264\n",
      "Step 178 | Loss = 0.049253\n",
      "Step 179 | Loss = 0.049243\n",
      "Step 180 | Loss = 0.049234\n",
      "Step 181 | Loss = 0.049227\n",
      "Step 182 | Loss = 0.049222\n",
      "Step 183 | Loss = 0.049219\n",
      "Step 184 | Loss = 0.049219\n",
      "Step 185 | Loss = 0.049220\n",
      "Step 186 | Loss = 0.049222\n",
      "Step 187 | Loss = 0.049226\n",
      "Step 188 | Loss = 0.049231\n",
      "Step 189 | Loss = 0.049237\n",
      "Step 190 | Loss = 0.049243\n",
      "Step 191 | Loss = 0.049249\n",
      "Step 192 | Loss = 0.049256\n",
      "Step 193 | Loss = 0.049261\n",
      "Step 194 | Loss = 0.049266\n",
      "Step 195 | Loss = 0.049270\n",
      "Step 196 | Loss = 0.049273\n",
      "Step 197 | Loss = 0.049275\n",
      "Step 198 | Loss = 0.049276\n",
      "Step 199 | Loss = 0.049276\n",
      "Step 200 | Loss = 0.049275\n",
      "Step 201 | Loss = 0.049273\n",
      "Step 202 | Loss = 0.049272\n",
      "Step 203 | Loss = 0.049270\n",
      "Step 204 | Loss = 0.049267\n",
      "Step 205 | Loss = 0.049265\n",
      "Step 206 | Loss = 0.049262\n",
      "Step 207 | Loss = 0.049260\n",
      "Step 208 | Loss = 0.049258\n",
      "Step 209 | Loss = 0.049256\n",
      "Step 210 | Loss = 0.049255\n",
      "Step 211 | Loss = 0.049255\n",
      "Step 212 | Loss = 0.049255\n",
      "Step 213 | Loss = 0.049255\n",
      "Step 214 | Loss = 0.049256\n",
      "Step 215 | Loss = 0.049256\n",
      "Step 216 | Loss = 0.049257\n",
      "Step 217 | Loss = 0.049258\n",
      "Step 218 | Loss = 0.049260\n",
      "Step 219 | Loss = 0.049261\n",
      "Step 220 | Loss = 0.049262\n",
      "Step 221 | Loss = 0.049264\n",
      "Step 222 | Loss = 0.049264\n",
      "Step 223 | Loss = 0.049265\n",
      "Step 224 | Loss = 0.049265\n",
      "Step 225 | Loss = 0.049265\n",
      "Step 226 | Loss = 0.049264\n",
      "Step 227 | Loss = 0.049263\n",
      "Step 228 | Loss = 0.049262\n",
      "Step 229 | Loss = 0.049261\n",
      "Step 230 | Loss = 0.049259\n",
      "Step 231 | Loss = 0.049258\n",
      "Step 232 | Loss = 0.049256\n",
      "Step 233 | Loss = 0.049255\n",
      "Step 234 | Loss = 0.049254\n",
      "Step 235 | Loss = 0.049253\n",
      "Step 236 | Loss = 0.049252\n",
      "Step 237 | Loss = 0.049251\n",
      "Step 238 | Loss = 0.049251\n",
      "Step 239 | Loss = 0.049251\n",
      "Step 240 | Loss = 0.049251\n",
      "Step 241 | Loss = 0.049251\n",
      "Step 242 | Loss = 0.049251\n",
      "Step 243 | Loss = 0.049251\n",
      "Step 244 | Loss = 0.049252\n",
      "Step 245 | Loss = 0.049253\n",
      "Step 246 | Loss = 0.049253\n",
      "Step 247 | Loss = 0.049254\n",
      "Step 248 | Loss = 0.049255\n",
      "Step 249 | Loss = 0.049256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m samples = generate_gaussian_data(mu_true, cov_true, n_samples, seed=seed+\u001b[32m1\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOttimizzazione in corso...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m result, history = \u001b[43moptimize_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_evolution\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m mu_hat = result[\u001b[33m\"\u001b[39m\u001b[33mmu\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     12\u001b[39m L_hat = result[\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36moptimize_lm\u001b[39m\u001b[34m(samples, n_steps, lr, eps, n_evolution, seed, verbose)\u001b[39m\n\u001b[32m     11\u001b[39m L = torch.tril(L_raw) + torch.eye(d) * \u001b[32m1e-3\u001b[39m\n\u001b[32m     12\u001b[39m loss = lm_loss(samples, mu, L, eps=eps, n_evolution=n_evolution, seed=seed)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m optimizer.step()\n\u001b[32m     15\u001b[39m loss_history.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CFM/cfm_env/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CFM/cfm_env/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CFM/cfm_env/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "d = 5\n",
    "n_samples = 500\n",
    "seed = 0\n",
    "\n",
    "mu_true, cov_true = generate_gaussian_params(d, seed=seed)\n",
    "samples = generate_gaussian_data(mu_true, cov_true, n_samples, seed=seed+1)\n",
    "\n",
    "print(\"Ottimizzazione in corso...\")\n",
    "result, history = optimize_lm(samples, n_steps=1000, lr=1e-2, eps=1e-2, n_evolution=10, seed=seed)\n",
    "\n",
    "mu_hat = result[\"mu\"]\n",
    "L_hat = result[\"L\"]\n",
    "prec_hat = L_hat @ L_hat.T\n",
    "\n",
    "cov_true_inv = torch.linalg.inv(cov_true)\n",
    "\n",
    "print(\"\\n---------- RISULTATI ----------\")\n",
    "print(\"mu_true:\", mu_true)\n",
    "print(\"mu_hat :\", mu_hat)\n",
    "print(\"\\ncov^-1_true:\\n\", cov_true_inv)\n",
    "print(\"prec_hat   :\\n\", prec_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098f90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_true: tensor([ 0.1541, -0.0293, -0.2179,  0.0568, -0.1085])\n",
      "mu_hat : tensor([ 0.0293,  0.2011, -0.2217, -0.0344, -0.3043])\n",
      "tensor(0.3397)\n",
      "\n",
      "cov^-1_true:\n",
      " tensor([[ 19.2456,  -9.9162,  16.2462,  -0.2003, -20.6090],\n",
      "        [ -9.9162,  22.0264,  -7.1457,   0.1632,  25.2464],\n",
      "        [ 16.2462,  -7.1457,  29.5353,  -0.4354, -20.0596],\n",
      "        [ -0.2003,   0.1632,  -0.4354,   2.6278,   1.5867],\n",
      "        [-20.6090,  25.2464, -20.0596,   1.5867,  43.5234]])\n",
      "tensor(85.8763)\n",
      "prec_hat   :\n",
      " tensor([[ 0.1948, -0.1337, -0.0559,  0.1384,  0.1696],\n",
      "        [-0.1337,  0.2940, -0.0022, -0.0866, -0.2713],\n",
      "        [-0.0559, -0.0022,  0.0567, -0.1305, -0.0087],\n",
      "        [ 0.1384, -0.0866, -0.1305,  1.0431,  0.1293],\n",
      "        [ 0.1696, -0.2713, -0.0087,  0.1293,  0.2988]])\n"
     ]
    }
   ],
   "source": [
    "print(\"mu_true:\", mu_true)\n",
    "print(\"mu_hat :\", mu_hat)\n",
    "print(torch.linalg.norm(mu_true - mu_hat))\n",
    "print(\"\\ncov^-1_true:\\n\", cov_true_inv)\n",
    "print(\"prec_hat   :\\n\", prec_hat)\n",
    "print(torch.linalg.norm(cov_true_inv - prec_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caabad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
