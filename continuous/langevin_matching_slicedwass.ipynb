{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "292e781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_samples(mu, cov, n_samples, seed=0):\n",
    "\n",
    "    d = mu.shape[0]\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    z = jax.random.normal(key, shape=(n_samples, d))\n",
    "\n",
    "    L = jnp.linalg.cholesky(cov)\n",
    "    samples = mu + z @ L.T\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf9f57a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_grad(sample, mu, cov):\n",
    "\n",
    "    precision = jnp.linalg.inv(cov)\n",
    "\n",
    "    return (sample - mu) @ precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8b40d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_langevin(samples, mu, cov, eps=1e-2, n_evolution=5, seed=0):\n",
    "    \n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    evolved_samples = samples\n",
    "\n",
    "    for i in range(n_evolution):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        grad = potential_grad(evolved_samples, mu, cov)\n",
    "        noise = jax.random.normal(subkey, shape=evolved_samples.shape)\n",
    "        evolved_samples = evolved_samples - eps * grad + jnp.sqrt(2 * eps) * noise\n",
    "\n",
    "    return evolved_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ced9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ott.tools import sinkhorn_divergence\n",
    "\n",
    "def compute_sinkhorn_divergence(x, y, epsilon=0.1):\n",
    "\n",
    "    divergence, sink = sinkhorn_divergence.sinkdiv(x, y, epsilon=epsilon)\n",
    "\n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c4e156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(samples, mu, cov):\n",
    "\n",
    "    evolved_samples = evolve_langevin(samples, mu, cov)\n",
    "\n",
    "    return compute_sinkhorn_divergence(samples, evolved_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a79946ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_optimize_with_tracking(samples, true_mu, true_cov, n_epochs=1000, lr=1e-2, seed=10, eps=1e-2, plot_every=2):\n",
    "    n, d = samples.shape\n",
    "\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key_mu, key_A = jax.random.split(key)\n",
    "\n",
    "    mu = jax.random.normal(key_mu, shape=(d,))\n",
    "    A = jax.random.normal(key_A, shape=(d, d))\n",
    "\n",
    "    params = {\n",
    "        \"mu\": mu,\n",
    "        \"A\": A\n",
    "    }\n",
    "\n",
    "    optimizer = optax.adam(lr)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    def loss_fn(params, samples):\n",
    "        mu = params[\"mu\"]\n",
    "        A = params[\"A\"]\n",
    "        cov = A.T @ A\n",
    "        return lm_loss(samples, mu, cov)\n",
    "\n",
    "    history = {\n",
    "        \"loss\": [],\n",
    "        \"mu_l2\": [],\n",
    "        \"cov_l2\": [],\n",
    "        \"cov_fro\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_val, grads = jax.value_and_grad(loss_fn)(params, samples)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        # Tracking\n",
    "        mu_val = params[\"mu\"]\n",
    "        A_val = params[\"A\"]\n",
    "        cov_val = A_val.T @ A_val\n",
    "\n",
    "        history[\"loss\"].append(loss_val)\n",
    "        history[\"mu_l2\"].append(jnp.linalg.norm(true_mu - mu_val))\n",
    "        history[\"cov_l2\"].append(jnp.linalg.norm(true_cov - cov_val, ord=2))\n",
    "        history[\"cov_fro\"].append(jnp.linalg.norm(true_cov - cov_val, ord='fro'))\n",
    "\n",
    "        if epoch % plot_every == 0 or epoch == n_epochs - 1:\n",
    "            print(f\"epoch {epoch} | loss = {loss_val:.6f}\")\n",
    "\n",
    "    final_mu = params[\"mu\"]\n",
    "    final_cov = params[\"A\"].T @ params[\"A\"]\n",
    "\n",
    "    return final_mu, final_cov, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f275876",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = jnp.array([0.0, 1.0, -1.0])\n",
    "\n",
    "cov = jnp.array([\n",
    "    [1.0, 0.5, 0.2],\n",
    "    [0.5, 1.0, 0.3],\n",
    "    [0.2, 0.3, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c891cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | loss = 0.049816\n",
      "epoch 2 | loss = 0.040484\n",
      "epoch 4 | loss = 0.038063\n",
      "epoch 6 | loss = 0.037248\n",
      "epoch 8 | loss = 0.036910\n",
      "epoch 10 | loss = 0.036734\n",
      "epoch 12 | loss = 0.036623\n",
      "epoch 14 | loss = 0.036537\n",
      "epoch 16 | loss = 0.036460\n",
      "epoch 18 | loss = 0.036385\n",
      "epoch 20 | loss = 0.036306\n",
      "epoch 22 | loss = 0.036209\n",
      "epoch 24 | loss = 0.036089\n",
      "epoch 26 | loss = 0.035952\n",
      "epoch 28 | loss = 0.035820\n",
      "epoch 30 | loss = 0.035714\n",
      "epoch 32 | loss = 0.035645\n",
      "epoch 34 | loss = 0.035611\n",
      "epoch 36 | loss = 0.035599\n",
      "epoch 38 | loss = 0.035589\n",
      "epoch 40 | loss = 0.035561\n",
      "epoch 42 | loss = 0.035513\n",
      "epoch 44 | loss = 0.035449\n",
      "epoch 46 | loss = 0.035378\n",
      "epoch 48 | loss = 0.035302\n",
      "epoch 50 | loss = 0.035227\n",
      "epoch 52 | loss = 0.035151\n",
      "epoch 54 | loss = 0.035074\n",
      "epoch 56 | loss = 0.034988\n",
      "epoch 58 | loss = 0.034891\n",
      "epoch 60 | loss = 0.034780\n",
      "epoch 62 | loss = 0.034659\n",
      "epoch 64 | loss = 0.034524\n",
      "epoch 66 | loss = 0.034377\n",
      "epoch 68 | loss = 0.034227\n",
      "epoch 70 | loss = 0.034079\n",
      "epoch 72 | loss = 0.033939\n",
      "epoch 74 | loss = 0.033815\n",
      "epoch 76 | loss = 0.033695\n",
      "epoch 78 | loss = 0.033564\n",
      "epoch 80 | loss = 0.033426\n",
      "epoch 82 | loss = 0.033305\n",
      "epoch 84 | loss = 0.033199\n",
      "epoch 86 | loss = 0.033091\n",
      "epoch 88 | loss = 0.032977\n",
      "epoch 90 | loss = 0.032868\n",
      "epoch 92 | loss = 0.032765\n",
      "epoch 94 | loss = 0.032648\n",
      "epoch 96 | loss = 0.032528\n",
      "epoch 98 | loss = 0.032411\n",
      "epoch 100 | loss = 0.032300\n",
      "epoch 102 | loss = 0.032186\n",
      "epoch 104 | loss = 0.032070\n",
      "epoch 106 | loss = 0.031950\n",
      "epoch 108 | loss = 0.031825\n",
      "epoch 110 | loss = 0.031698\n",
      "epoch 112 | loss = 0.031571\n",
      "epoch 114 | loss = 0.031444\n",
      "epoch 116 | loss = 0.031317\n",
      "epoch 118 | loss = 0.031186\n",
      "epoch 120 | loss = 0.031051\n",
      "epoch 122 | loss = 0.030913\n",
      "epoch 124 | loss = 0.030774\n",
      "epoch 126 | loss = 0.030646\n",
      "epoch 128 | loss = 0.030543\n",
      "epoch 130 | loss = 0.030480\n",
      "epoch 132 | loss = 0.030459\n",
      "epoch 134 | loss = 0.030465\n",
      "epoch 136 | loss = 0.030480\n",
      "epoch 138 | loss = 0.030489\n",
      "epoch 140 | loss = 0.030487\n",
      "epoch 142 | loss = 0.030470\n",
      "epoch 144 | loss = 0.030445\n",
      "epoch 146 | loss = 0.030420\n",
      "epoch 148 | loss = 0.030404\n",
      "epoch 150 | loss = 0.030399\n",
      "epoch 152 | loss = 0.030400\n",
      "epoch 154 | loss = 0.030401\n",
      "epoch 156 | loss = 0.030398\n",
      "epoch 158 | loss = 0.030392\n",
      "epoch 160 | loss = 0.030390\n",
      "epoch 162 | loss = 0.030388\n",
      "epoch 164 | loss = 0.030386\n",
      "epoch 166 | loss = 0.030386\n",
      "epoch 168 | loss = 0.030385\n",
      "epoch 170 | loss = 0.030385\n",
      "epoch 172 | loss = 0.030387\n",
      "epoch 174 | loss = 0.030387\n",
      "epoch 176 | loss = 0.030387\n",
      "epoch 178 | loss = 0.030386\n",
      "epoch 180 | loss = 0.030385\n",
      "epoch 182 | loss = 0.030384\n",
      "epoch 184 | loss = 0.030384\n",
      "epoch 186 | loss = 0.030384\n",
      "epoch 188 | loss = 0.030383\n",
      "epoch 190 | loss = 0.030383\n",
      "epoch 192 | loss = 0.030384\n",
      "epoch 194 | loss = 0.030382\n",
      "epoch 196 | loss = 0.030382\n",
      "epoch 198 | loss = 0.030384\n",
      "epoch 200 | loss = 0.030383\n",
      "epoch 202 | loss = 0.030382\n",
      "epoch 204 | loss = 0.030384\n",
      "epoch 206 | loss = 0.030382\n",
      "epoch 208 | loss = 0.030382\n",
      "epoch 210 | loss = 0.030382\n",
      "epoch 212 | loss = 0.030382\n",
      "epoch 214 | loss = 0.030384\n",
      "epoch 216 | loss = 0.030382\n",
      "epoch 218 | loss = 0.030382\n",
      "epoch 220 | loss = 0.030384\n",
      "epoch 222 | loss = 0.030382\n",
      "epoch 224 | loss = 0.030382\n",
      "epoch 226 | loss = 0.030384\n",
      "epoch 228 | loss = 0.030382\n",
      "epoch 230 | loss = 0.030384\n",
      "epoch 232 | loss = 0.030382\n",
      "epoch 234 | loss = 0.030382\n",
      "epoch 236 | loss = 0.030384\n",
      "epoch 238 | loss = 0.030382\n",
      "epoch 240 | loss = 0.030382\n",
      "epoch 242 | loss = 0.030384\n",
      "epoch 244 | loss = 0.030382\n",
      "epoch 246 | loss = 0.030382\n",
      "epoch 248 | loss = 0.030384\n",
      "epoch 250 | loss = 0.030384\n",
      "epoch 252 | loss = 0.030382\n",
      "epoch 254 | loss = 0.030382\n",
      "epoch 256 | loss = 0.030384\n",
      "epoch 258 | loss = 0.030382\n",
      "epoch 260 | loss = 0.030382\n",
      "epoch 262 | loss = 0.030384\n",
      "epoch 264 | loss = 0.030382\n",
      "epoch 266 | loss = 0.030382\n",
      "epoch 268 | loss = 0.030384\n",
      "epoch 270 | loss = 0.030382\n",
      "epoch 272 | loss = 0.030382\n",
      "epoch 274 | loss = 0.030384\n",
      "epoch 276 | loss = 0.030384\n",
      "epoch 278 | loss = 0.030382\n"
     ]
    }
   ],
   "source": [
    "samples = generate_gaussian_samples(mu, cov, n_samples=2000, seed=0)\n",
    "\n",
    "final_mu, final_cov, history = lm_optimize_with_tracking(samples, mu, cov, n_epochs=800, lr=0.05, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] \n",
      "\n",
      " [0.0171 0.0117] 0.020761693 \n",
      "\n",
      "\n",
      "\n",
      "[[2.001 2.   ]\n",
      " [2.    2.001]] \n",
      "\n",
      " [[3.5716 3.564 ]\n",
      " [3.564  3.5769]] 3.1372166 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jnp.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(jnp.round(mu, 4), \"\\n\\n\", jnp.round(final_mu, 4), jnp.linalg.norm(mu - final_mu), \"\\n\\n\\n\")\n",
    "print(jnp.round(cov, 4), \"\\n\\n\", jnp.round(final_cov, 4), jnp.linalg.norm(cov - final_cov), \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1af646",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_np = {k: np.array(v) for k, v in history.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34daabcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_np[\"loss\"], label=\"langevin matching loss\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"loss evolution\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_np[\"mu_l2\"], label=r\"$\\| \\mu - \\hat{\\mu} \\|_2$\")\n",
    "plt.plot(history_np[\"cov_l2\"], label=r\"$\\| \\Sigma - \\hat{\\Sigma} \\|_2$\")\n",
    "plt.plot(history_np[\"cov_fro\"], label=r\"$\\| \\Sigma - \\hat{\\Sigma} \\|_F$\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.title(\"reconstruction error\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
