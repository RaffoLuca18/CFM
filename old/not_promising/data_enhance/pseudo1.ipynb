{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e8e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import data_gen as dg\n",
    "import ising as isg\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f3db7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pseudolikelihood_site(samples, i, h, J, beta=1.0):\n",
    "\n",
    "    M = samples.shape[0]\n",
    "    mus = jnp.arange(M)\n",
    "    logps = isg.compute_logp_all_mu(mus, samples, i, h, J, beta)\n",
    "    \n",
    "    return jnp.mean(logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae175301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_log_pseudolikelihood(samples, h, J, beta=1.0):\n",
    "    \n",
    "    d = samples.shape[1]\n",
    "    sites = jnp.arange(d)\n",
    "    site_logps = jax.vmap(log_pseudolikelihood_site, in_axes=(None, 0, None, None, None))(samples, sites, h, J, beta)\n",
    "\n",
    "    return jnp.sum(site_logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb83d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl_loss(samples, h, J, beta=1.0):\n",
    "    \n",
    "    return -total_log_pseudolikelihood(samples, h, J, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3e55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl_gradients(samples, h, J, beta):\n",
    "\n",
    "    grad_loss = jax.grad(pl_loss, argnums=(1, 2))\n",
    "\n",
    "    return grad_loss(samples, h, J, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8bda162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl_step(params, opt_state, samples, optimizer, beta):\n",
    "    \n",
    "    grad_h, grad_J = pl_gradients(samples, params[\"h\"], params[\"J\"], beta)\n",
    "\n",
    "    grads = {\n",
    "        \"h\": grad_h,\n",
    "        \"J\": grad_J\n",
    "    }\n",
    "\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    J = params[\"J\"]\n",
    "    J = 0.5 * (J + J.T)\n",
    "    J = J - jnp.diag(J)\n",
    "    params[\"J\"] = J\n",
    "\n",
    "    return params, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bda17977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_pl_enhanced(samples, h_init, J_init, n_steps=1000, lr=1e-2,\n",
    "                          beta=1.0, delta=1e-2, n_enhance=100, seed=0):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key_enhance = key\n",
    "\n",
    "    params = {\n",
    "        \"h\": h_init,\n",
    "        \"J\": J_init\n",
    "    }\n",
    "\n",
    "    optimizer = optax.adam(lr)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    prev_params = params\n",
    "    last_enhance_step = 0\n",
    "    counter = 0\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        params, opt_state = pl_step(params, opt_state, samples, optimizer, beta)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            loss_val = pl_loss(samples, params[\"h\"], params[\"J\"], beta)\n",
    "            print(f\"Step {step:4d} | Loss: {loss_val:.6f}\")\n",
    "\n",
    "        delta_h = jnp.linalg.norm(params[\"h\"] - prev_params[\"h\"])\n",
    "        delta_J = jnp.linalg.norm(params[\"J\"] - prev_params[\"J\"])\n",
    "        delta_theta = jnp.sqrt(delta_h**2 + delta_J**2)\n",
    "\n",
    "        if delta_theta < delta and (step - last_enhance_step) >= n_enhance:\n",
    "            key_enhance, subkey = jax.random.split(key_enhance)\n",
    "            keys = jax.random.split(subkey, samples.shape[0])\n",
    "            new_samples = dg.apply_glauber_to_all(keys, samples, params[\"h\"], params[\"J\"], beta)\n",
    "            samples = jnp.concatenate([samples, new_samples], axis=0)\n",
    "            last_enhance_step = step\n",
    "            print(f\" ↳ Enhanced at step {step}, Δθ = {delta_theta:.2e}, total samples = {samples.shape[0]}\")\n",
    "\n",
    "        prev_params = params\n",
    "\n",
    "    return params[\"h\"], params[\"J\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bafff37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:[-0.10000000149011612, 0.020000001415610313, -0.24000000953674316, -0.2600000202655792, 0.020000001415610313]\n",
      "[0.7350000143051147, -0.503000020980835, -0.5509999990463257, -0.8060000538825989, -0.6200000047683716]\n",
      "0.2369999885559082\n",
      "\n",
      "step 5000:[0.9800000190734863, -0.9800000190734863, -0.9800000190734863, 0.7200000286102295, -0.9800000190734863]\n",
      "[0.9980000257492065, -1.0, -0.9720000624656677, 0.7860000133514404, -0.9980000257492065]\n",
      "0.010799991898238659\n",
      "\n",
      "step 10000:[0.9600000381469727, -0.9600000381469727, -0.8800000548362732, 0.7000000476837158, -0.9600000381469727]\n",
      "[0.9980000257492065, -1.0, -0.9700000286102295, 0.7540000081062317, -0.9980000257492065]\n",
      "0.0151999955996871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = 5\n",
    "\n",
    "n_init = 100\n",
    "\n",
    "n_replicas = 500\n",
    "\n",
    "h, J = dg.generate_ising_params(d, sigma_h=1, sigma_J=0.5, seed=0)\n",
    "\n",
    "samples = dg.generate_ising_data(n_init, n_replicas, h=h, J=J, n_steps_equil=10000, n_steps_final=1000, n_prints = 5000, beta=1, seed=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "506c8293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True h: [ 1.0040143 -0.9063372 -0.7481722 -1.1713669 -0.8712328]\n",
      "True J: [[ 0.         -1.6071162   0.00798594  0.04462025 -0.93398416]\n",
      " [-1.6071162   0.          0.63058895 -1.2504177   0.90409863]\n",
      " [ 0.00798594  0.63058895  0.         -0.40897474  0.47866577]\n",
      " [ 0.04462025 -1.2504177  -0.40897474  0.         -0.5732635 ]\n",
      " [-0.93398416  0.90409863  0.47866577 -0.5732635   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"True h:\", h)\n",
    "print(\"True J:\", J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "797a1f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0 | Loss: 1.983862\n",
      "Step  100 | Loss: 0.435885\n",
      "Step  200 | Loss: 0.431507\n",
      "Step  300 | Loss: 0.428691\n",
      "Step  400 | Loss: 0.427673\n",
      "Step  500 | Loss: 0.429248\n",
      "Step  600 | Loss: 0.429540\n",
      "Step  700 | Loss: 0.427864\n",
      "Step  800 | Loss: 0.428851\n",
      "Step  900 | Loss: 0.427160\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "h0 = 0.1 * jax.random.normal(key, shape=(d,))\n",
    "J0 = 0.1 * jax.random.normal(key, shape=(d, d))\n",
    "J0 = 0.5 * (J0 + J0.T)\n",
    "J0 = J0 - jnp.diag(jnp.diag(J0))\n",
    "\n",
    "h_est, J_est = optimize_pl_enhanced(samples, h0, J0, n_steps=1000, lr=0.1, beta=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e63acc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated h: [ 0.83900005 -1.0710001  -0.90500003 -1.4670001  -1.136     ]\n",
      "estimated J: [[ 0.         -1.9660001  -0.675      -0.94000006 -1.09      ]\n",
      " [-1.9660001   0.          0.23       -2.0640001   0.83500004]\n",
      " [-0.679       0.22600001  0.         -0.54200006  0.052     ]\n",
      " [-0.89800006 -2.023      -0.49600002  0.         -0.96000004]\n",
      " [-1.093       0.832       0.053      -1.005       0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "loss h: 0.48682067 loss J: 2.3281848\n"
     ]
    }
   ],
   "source": [
    "print(\"estimated h:\", jnp.round(h_est, 3))\n",
    "print(\"estimated J:\", jnp.round(J_est, 3))\n",
    "print(\"\\n\\n\")\n",
    "print(\"loss h:\", jnp.linalg.norm(h - h_est), \"loss J:\", jnp.linalg.norm(J - J_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb4560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9176ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
