{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0e32da",
   "metadata": {},
   "source": [
    "# framework:\n",
    "    1. we have n_spins random variables, whose range is the real line\n",
    "    2. the real distribution is a multivariate gaussian p_true ~ N(true_mean, true_cov)\n",
    "    3. we see n_data samples\n",
    "    4. we use score matching on the family of gaussians to infer true_mean, true_cov\n",
    "\n",
    "NB this is a toy model, for gaussians score matching is equivalent to MLE, but we use a gradient descent here for the sake of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab890895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrand\n",
    "from jax import grad, jit, vmap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf71e4d",
   "metadata": {},
   "source": [
    "### data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d0b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "key = jrand.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d51417",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spins = 2\n",
    "n_data = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46cd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jrand.split(key)\n",
    "true_mean = jrand.uniform(subkey, shape = n_spins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b5cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jrand.split(key)\n",
    "temp = jrand.uniform(subkey, shape = [n_spins, n_spins], minval = 0)\n",
    "true_cov = temp @ temp.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544f5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jrand.split(key)\n",
    "data = jrand.multivariate_normal(subkey, true_mean, true_cov, shape = n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e3ecdf",
   "metadata": {},
   "source": [
    "### densities and derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a680b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_density(x, mean, cov, precision):\n",
    "    n_spins = len(mean)\n",
    "    det_cov = jnp.linalg.det(cov)\n",
    "    norm_const = 1.0 / jnp.sqrt((2 * jnp.pi)**n_spins * det_cov)\n",
    "\n",
    "    diff = x - mean\n",
    "    exponent = -0.5 * jnp.dot(jnp.dot(diff, precision), diff)\n",
    "    density = norm_const * jnp.exp(exponent)\n",
    "\n",
    "    return density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0009f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_gaussian(x, mean, precision):\n",
    "    diff = x - mean\n",
    "    return -jnp.dot(precision, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76225390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lapl_log_gaussian(precision):\n",
    "    return -jnp.trace(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119955df",
   "metadata": {},
   "source": [
    "### score matching\n",
    "\n",
    "remember we assume gaussianity...\n",
    "\n",
    "- the score is: $\\nabla_x \\log p_\\theta(x) = -\\Sigma^{-1}(x - \\mu)$  \n",
    "- the laplacian is: $\\Delta_x \\log p_\\theta(x) = -\\operatorname{Tr}(\\Sigma^{-1})$\n",
    "\n",
    "plugging into the score matching loss gives:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}_{x \\sim \\text{data}} \\left[ \\frac{1}{2} \\| \\Sigma^{-1}(x - \\mu) \\|^2 - \\operatorname{Tr}(\\Sigma^{-1}) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### parametrization\n",
    "\n",
    "to ensure that the covariance matrix ($\\Sigma$) remains positive definite, we parametrize it using its Cholesky decomposition:\n",
    "\n",
    "- the diagonal entries of the Cholesky factor ($L$) are exponentiated from `log_sigma_diag`  \n",
    "- the lower-triangular off-diagonal entries are stored in `cholesky_factor_offdiag`  \n",
    "- the full matrix is reconstructed as $\\Sigma = L L^\\top$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52431b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_matching_loss(params_raw, data):\n",
    "    # extract mean vector from parameters\n",
    "    mean = params_raw['mean']\n",
    "    \n",
    "    # extract log of diagonal of cholesky factor\n",
    "    log_sigma_diag = params_raw['log_sigma_diag']\n",
    "    \n",
    "    # extract off-diagonal entries of cholesky factor\n",
    "    cholesky_factor_offdiag = params_raw['cholesky_factor_offdiag']\n",
    "\n",
    "    # get data dimensionality\n",
    "    n_dim = len(mean)\n",
    "    \n",
    "    # initialize cholesky factor matrix L\n",
    "    L = jnp.zeros((n_dim, n_dim), dtype=data.dtype)\n",
    "    \n",
    "    # set diagonal entries using exponentiated log_sigma_diag\n",
    "    L = L.at[jnp.diag_indices(n_dim)].set(jnp.exp(log_sigma_diag))\n",
    "    \n",
    "    # if dimension > 1, set lower triangular off-diagonal entries\n",
    "    if n_dim > 1:\n",
    "        lower_tri_indices = jnp.tril_indices(n_dim, k=-1)\n",
    "        assert cholesky_factor_offdiag.shape[0] == (n_dim * (n_dim - 1) // 2)\n",
    "        L = L.at[lower_tri_indices].set(cholesky_factor_offdiag)\n",
    "\n",
    "    # compute covariance matrix from cholesky factor\n",
    "    cov = L @ L.T \n",
    "    \n",
    "    # compute precision matrix (inverse of covariance)\n",
    "    precision = jnp.linalg.inv(cov)\n",
    "\n",
    "    # initialize total loss\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # get number of data points\n",
    "    N = data.shape[0]\n",
    "\n",
    "    # loop over each data point\n",
    "    for i in range(N):\n",
    "        x_i = data[i, :]\n",
    "        \n",
    "        # compute score (gradient of log-density)\n",
    "        score_val = grad_log_gaussian(x_i, mean, precision)\n",
    "        \n",
    "        # compute squared norm of score\n",
    "        term1 = 0.5 * jnp.sum(score_val**2)\n",
    "\n",
    "        # compute laplacian of log-density (does not depend on x)\n",
    "        term2 = lapl_log_gaussian(precision)\n",
    "        \n",
    "        # add both terms to total loss\n",
    "        total_loss += (term1 + term2)\n",
    "    \n",
    "    # return average loss over all data points\n",
    "    return total_loss / N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70660eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_off_diag_elements = n_spins * (n_spins - 1) // 2\n",
    "\n",
    "params_raw = {\n",
    "    'mean': jnp.zeros(n_spins),\n",
    "    'log_sigma_diag': jnp.zeros(n_spins),\n",
    "    'cholesky_factor_offdiag': jnp.zeros(num_off_diag_elements)\n",
    "}\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_iterations = 4000\n",
    "\n",
    "loss_and_grad_fn = jax.value_and_grad(score_matching_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "582ec6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the optimization \n",
      "\n",
      "\n",
      "\n",
      "iteration 500/4000\n",
      "\n",
      "  estimated Mean: [0.05451116 0.4317398 ]\n",
      "\n",
      "  estimated Covariance:\n",
      "[[0.9454663  0.33368996]\n",
      " [0.33368996 0.19797575]]\n",
      "\n",
      "\n",
      "\n",
      "iteration 1000/4000\n",
      "\n",
      "  estimated Mean: [0.1948535  0.48528156]\n",
      "\n",
      "  estimated Covariance:\n",
      "[[0.90306747 0.31750458]\n",
      " [0.31750458 0.19178313]]\n",
      "\n",
      "\n",
      "\n",
      "iteration 1500/4000\n",
      "\n",
      "  estimated Mean: [0.2936692 0.5229742]\n",
      "\n",
      "  estimated Covariance:\n",
      "[[0.84912366 0.29692477]\n",
      " [0.29692477 0.18393223]]\n",
      "\n",
      "\n",
      "\n",
      "iteration 2000/4000\n",
      "\n",
      "  estimated Mean: [0.3574913 0.5473177]\n",
      "\n",
      "  estimated Covariance:\n",
      "[[0.82127243 0.28630427]\n",
      " [0.28630427 0.17988238]]\n",
      "\n",
      "\n",
      "\n",
      "iteration 2500/4000\n",
      "\n",
      "  estimated Mean: [0.3951242  0.56167203]\n",
      "\n",
      "  estimated Covariance:\n",
      "[[0.81047714 0.28218898]\n",
      " [0.28218898 0.17831337]]\n",
      "\n",
      "\n",
      "\n",
      "iteration 3000/4000\n",
      "\n",
      "  estimated Mean: [0.4163038 0.5697507]\n",
      "\n",
      "  estimated Covariance:\n",
      "[[0.8068265  0.28079757]\n",
      " [0.28079757 0.17778319]]\n",
      "\n",
      "\n",
      "\n",
      "iteration 3500/4000\n",
      "\n",
      "  estimated Mean: [0.42801183 0.57421666]\n",
      "\n",
      "  estimated Covariance:\n",
      "[[0.80566853 0.28035626]\n",
      " [0.28035626 0.17761505]]\n",
      "\n",
      "\n",
      "\n",
      "iteration 4000/4000\n",
      "\n",
      "  estimated Mean: [0.43444502 0.57667047]\n",
      "\n",
      "  estimated Covariance:\n",
      "[[0.8053118  0.28022033]\n",
      " [0.28022033 0.17756322]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize list to store loss values\n",
    "loss_history = []\n",
    "\n",
    "# print start message\n",
    "print(\"starting the optimization \\n\\n\\n\")\n",
    "\n",
    "# main optimization loop\n",
    "for i in range(num_iterations):\n",
    "    # evaluate loss and its gradient\n",
    "    loss_val, grads = loss_and_grad_fn(params_raw, data)\n",
    "    \n",
    "    # store current loss value\n",
    "    loss_history.append(loss_val.item())\n",
    "\n",
    "    # gradient descent update on parameters\n",
    "    params_raw = jax.tree.map(lambda p, g: p - learning_rate * g, params_raw, grads)\n",
    "\n",
    "    # print diagnostics every 500 iterations\n",
    "    if (i + 1) % 500 == 0:\n",
    "        # extract current estimates\n",
    "        estimated_mean = params_raw['mean']\n",
    "        estimated_log_sigma_diag = params_raw['log_sigma_diag']\n",
    "        estimated_cholesky_factor_offdiag = params_raw['cholesky_factor_offdiag']\n",
    "\n",
    "        # reconstruct lower triangular cholesky factor\n",
    "        L_est = jnp.zeros((n_spins, n_spins))\n",
    "        L_est = L_est.at[jnp.diag_indices(n_spins)].set(jnp.exp(estimated_log_sigma_diag))\n",
    "        \n",
    "        # set off-diagonal lower triangular elements if dimension > 1\n",
    "        if n_spins > 1:\n",
    "            lower_tri_indices = jnp.tril_indices(n_spins, k=-1)\n",
    "            L_est = L_est.at[lower_tri_indices].set(estimated_cholesky_factor_offdiag)\n",
    "\n",
    "        # compute estimated covariance matrix\n",
    "        estimated_cov = L_est @ L_est.T\n",
    "\n",
    "        # print estimated mean and covariance\n",
    "        print(f\"iteration {i+1}/{num_iterations}\\n\")\n",
    "        print(f\"  estimated Mean: {estimated_mean}\\n\")\n",
    "        print(f\"  estimated Covariance:\\n{estimated_cov}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97d4bbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([0.40364635, 0.548895  ], dtype=float32),\n",
       " Array([[0.7159164 , 0.21423683],\n",
       "        [0.21423683, 0.12737265]], dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_mean, true_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8022ffbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.03079867, 0.02777547], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_mean-true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81125408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.0893954 , 0.0659835 ],\n",
       "       [0.0659835 , 0.05019057]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_cov-true_cov"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".exp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
